\hypertarget{namespacehtmldata}{}\section{htmldata Namespace Reference}
\label{namespacehtmldata}\index{htmldata@{htmldata}}
\subsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \hyperlink{classhtmldata_1_1___h_t_m_l_tag}{\+\_\+\+H\+T\+M\+L\+Tag}
\item 
class \hyperlink{classhtmldata_1_1___text_tag}{\+\_\+\+Text\+Tag}
\item 
class \hyperlink{classhtmldata_1_1_u_r_l_match}{U\+R\+L\+Match}
\end{DoxyCompactItemize}
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{namespacehtmldata_a3d5a0fd3ca07d74ca1fcd1d825102307}{tagextract} (doc)
\item 
def \hyperlink{namespacehtmldata_abea91d7c71df2f825ea2fd1ab11387cd}{tagjoin} (L)
\item 
def \hyperlink{namespacehtmldata_a849b6b90a5518672e5494c58e7bee65c}{urlextract} (doc, siteurl=None, mimetype=\textquotesingle{}text/html\textquotesingle{})
\item 
def \hyperlink{namespacehtmldata_a281fd13bfba54efd22dfe1e901181648}{urljoin} (\hyperlink{lib_2expat_8h_a755339d27872b13735c2cab829e47157}{s}, L)
\item 
def \hyperlink{namespacehtmldata_ab52faf44a3a2bcdf3bf87668c93cba00}{examples} ()
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}Manipulate HTML or XHTML documents.

Version 1.1.1.  This source code has been placed in the
public domain by Connelly Barnes.

Features:

 - Translate HTML back and forth to data structures.
 This allows you to read and write HTML documents
 programmably, with much flexibility.
 - Extract and modify URLs in an HTML document.
 - Compatible with Python 2.0 - 2.5.

See the L{examples} for a quick start.\end{DoxyVerb}
 

\subsection{Function Documentation}
\index{htmldata@{htmldata}!examples@{examples}}
\index{examples@{examples}!htmldata@{htmldata}}
\subsubsection[{\texorpdfstring{examples()}{examples()}}]{\setlength{\rightskip}{0pt plus 5cm}def htmldata.\+examples (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\hypertarget{namespacehtmldata_ab52faf44a3a2bcdf3bf87668c93cba00}{}\label{namespacehtmldata_ab52faf44a3a2bcdf3bf87668c93cba00}
\begin{DoxyVerb}Examples of the C{htmldata} module.

Example 1:
Print all absolutized URLs from Google.

Here we use L{urlextract} to obtain all URLs in the document.

 >>> import urllib2, htmldata
 >>> url = 'http://www.google.com/'
 >>> contents = urllib2.urlopen(url).read()
 >>> for u in htmldata.urlextract(contents, url):
 ...   print u.url
 ...
 http://www.google.com/images/logo.gif
 http://www.google.com/search
 (More output)

Note that the second argument to L{urlextract} causes the
URLs to be made absolute with respect to that base URL.

Example 2:
Print all image URLs from Google in relative form.


 >>> import urllib2, htmldata
 >>> url = 'http://www.google.com/'
 >>> contents = urllib2.urlopen(url).read()
 >>> for u in htmldata.urlextract(contents):
 ...   if u.tag_name == 'img':
 ...     print u.url
 ...
 /images/logo.gif

Equivalently, one can use L{tagextract}, and look for occurrences
of C{<img>} tags. The L{urlextract} function is mostly a convenience
function for when one wants to extract and/or modify all URLs in a
document.

Example 3:
Replace all C{<a href>} links on Google with the Microsoft web page.

Here we use L{tagextract} to turn the HTML into a data structure,
and then loop over the in-order list of tags (items which are not
tuples are plain text, which is ignored).

 >>> import urllib2, htmldata
 >>> url = 'http://www.google.com/'
 >>> contents = urllib2.urlopen(url).read()
 >>> L = htmldata.tagextract(contents)
 >>> for item in L:
 ...   if isinstance(item, tuple) and item[0] == 'a':
 ...     # It's an HTML <a> tag!  Give it an href=.
 ...     item[1]['href'] = 'http://www.microsoft.com/'
 ...
 >>> htmldata.tagjoin(L)
 (Microsoftized version of Google)

Example 4:
Make all URLs on an HTML document be absolute.

 >>> import urllib2, htmldata
 >>> url = 'http://www.google.com/'
 >>> contents = urllib2.urlopen(url).read()
 >>> htmldata.urljoin(htmldata.urlextract(contents, url))
 (Google HTML page with absolute URLs)

Example 5:
Properly quote all HTML tag values for pedants.

 >>> import urllib2, htmldata
 >>> url = 'http://www.google.com/'
 >>> contents = urllib2.urlopen(url).read()
 >>> htmldata.tagjoin(htmldata.tagextract(contents))
 (Properly quoted version of the original HTML)

Example 6:
Modify all URLs in a document so that they are appended
to our proxy CGI script C{http://mysite.com/proxy.cgi}.

 >>> import urllib2, htmldata
 >>> url = 'http://www.google.com/'
 >>> contents = urllib2.urlopen(url).read()
 >>> proxy_url = 'http://mysite.com/proxy.cgi?url='
 >>> L = htmldata.urlextract(contents)
 >>> for u in L:
 ...   u.url = proxy_url + u.url
 ...
 >>> htmldata.urljoin(L)
 (Document with all URLs wrapped in our proxy script)

Example 7:
Download all images from a website.

 >>> import urllib, htmldata, time
 >>> url = 'http://www.google.com/'
 >>> contents = urllib.urlopen(url).read()
 >>> for u in htmldata.urlextract(contents, url):
 ...   if u.tag_name == 'img':
 ...     filename = urllib.quote_plus(u.url)
 ...     urllib.urlretrieve(u.url, filename)
 ...     time.sleep(0.5)
 ...
 (Images are downloaded to the current directory)

Many sites will protect against bandwidth-draining robots by
checking the HTTP C{Referer} [sic] and C{User-Agent} fields.
To circumvent this, one can create a C{urllib2.Request} object
with a legitimate C{Referer} and a C{User-Agent} such as
C{"Mozilla/4.0 (compatible; MSIE 5.5)"}.  Then use
C{urllib2.urlopen} to download the content.  Be warned that some
website operators will respond to rapid robot requests by banning
the offending IP address.\end{DoxyVerb}
 

Definition at line 977 of file htmldata.\+py.

\index{htmldata@{htmldata}!tagextract@{tagextract}}
\index{tagextract@{tagextract}!htmldata@{htmldata}}
\subsubsection[{\texorpdfstring{tagextract(doc)}{tagextract(doc)}}]{\setlength{\rightskip}{0pt plus 5cm}def htmldata.\+tagextract (
\begin{DoxyParamCaption}
\item[{}]{doc}
\end{DoxyParamCaption}
)}\hypertarget{namespacehtmldata_a3d5a0fd3ca07d74ca1fcd1d825102307}{}\label{namespacehtmldata_a3d5a0fd3ca07d74ca1fcd1d825102307}
\begin{DoxyVerb}Convert HTML to data structure.

Returns a list.  HTML tags become C{(name, keyword_dict)} tuples
within the list, while plain text becomes strings within the
list.  All tag names are lowercased and stripped of whitespace.
Tags which end with forward slashes have a single forward slash
placed at the end of their name, to indicate that they are XML
unclosed tags.

Example:

 >>> tagextract('<img src=hi.gif alt="hi">foo<br><br/></body>')
 [('img', {'src': 'hi.gif', 'alt': 'hi'}), 'foo',
  ('br', {}), ('br/', {}), ('/body', {})]

Text between C{'<script>'} and C{'<style>'} is rendered directly to
plain text. This prevents rogue C{'<'} or C{'>'} characters from
interfering with parsing.

 >>> tagextract('<script type="a"><blah>var x; </script>')
 [('script', {'type': 'a'}), '<blah>var x; ', ('/script', {})]

Comment strings and XML directives are rendered as a single long
tag with no attributes.  The case of the tag "name" is not changed:

 >>> tagextract('<!-- blah -->')
 [('!-- blah --', {})]
 >>> tagextract('<?xml version="1.0" encoding="utf-8" ?>')
 [('?xml version="1.0" encoding="utf-8" ?', {})]
 >>> tagextract('<!DOCTYPE html PUBLIC etc...>')
 [('!DOCTYPE html PUBLIC etc...', {})]

Greater-than and less-than characters occuring inside comments or
CDATA blocks are correctly kept as part of the block:

 >>> tagextract('<!-- <><><><>>..> -->')
 [('!-- <><><><>>..> --', {})]
 >>> tagextract('<!CDATA[[><>><>]<> ]]>')
 [('!CDATA[[><>><>]<> ]]', {})]

Note that if one modifies these tags, it is important to retain the
C{"--"} (for comments) or C{"]]"} (for C{CDATA}) at the end of the
tag name, so that output from L{tagjoin} will be correct HTML/XHTML.\end{DoxyVerb}
 

Definition at line 64 of file htmldata.\+py.

\index{htmldata@{htmldata}!tagjoin@{tagjoin}}
\index{tagjoin@{tagjoin}!htmldata@{htmldata}}
\subsubsection[{\texorpdfstring{tagjoin(\+L)}{tagjoin(L)}}]{\setlength{\rightskip}{0pt plus 5cm}def htmldata.\+tagjoin (
\begin{DoxyParamCaption}
\item[{}]{L}
\end{DoxyParamCaption}
)}\hypertarget{namespacehtmldata_abea91d7c71df2f825ea2fd1ab11387cd}{}\label{namespacehtmldata_abea91d7c71df2f825ea2fd1ab11387cd}
\begin{DoxyVerb}Convert data structure back to HTML.

This reverses the L{tagextract} function.

More precisely, if an HTML string is turned into a data structure,
then back into HTML, the resulting string will be functionally
equivalent to the original HTML.

 >>> tagjoin(tagextract(s))
 (string that is functionally equivalent to s)

Three changes are made to the HTML by L{tagjoin}: tags are
lowercased, C{key=value} pairs are sorted, and values are placed in
double-quotes.
\end{DoxyVerb}
 

Definition at line 127 of file htmldata.\+py.

\index{htmldata@{htmldata}!urlextract@{urlextract}}
\index{urlextract@{urlextract}!htmldata@{htmldata}}
\subsubsection[{\texorpdfstring{urlextract(doc, siteurl=\+None, mimetype=\textquotesingle{}text/html\textquotesingle{})}{urlextract(doc, siteurl=None, mimetype='text/html')}}]{\setlength{\rightskip}{0pt plus 5cm}def htmldata.\+urlextract (
\begin{DoxyParamCaption}
\item[{}]{doc, }
\item[{}]{siteurl = {\ttfamily None}, }
\item[{}]{mimetype = {\ttfamily \textquotesingle{}text/html\textquotesingle{}}}
\end{DoxyParamCaption}
)}\hypertarget{namespacehtmldata_a849b6b90a5518672e5494c58e7bee65c}{}\label{namespacehtmldata_a849b6b90a5518672e5494c58e7bee65c}
\begin{DoxyVerb}Extract URLs from HTML or stylesheet.

Extracts only URLs that are linked to or embedded in the document.
Ignores plain text URLs that occur in the non-HTML part of the
document.

Returns a list of L{URLMatch} objects.

 >>> L = urlextract('<img src="a.gif"><a href="www.google.com">')
 >>> L[0].url
 'a.gif'
 >>> L[1].url
 'www.google.com'

If C{siteurl} is specified, all URLs are made into absolute URLs
by assuming that C{doc} is located at the URL C{siteurl}.

 >>> doc = '<img src="a.gif"><a href="/b.html">'
 >>> L = urlextract(doc, 'http://www.python.org/~guido/')
 >>> L[0].url
 'http://www.python.org/~guido/a.gif'
 >>> L[1].url
 'http://www.python.org/b.html'

If C{mimetype} is C{"text/css"}, the document will be parsed
as a stylesheet.

If a stylesheet is embedded inside an HTML document, then
C{urlextract} will extract the URLs from both the HTML and the
stylesheet.
\end{DoxyVerb}
 

Definition at line 786 of file htmldata.\+py.

\index{htmldata@{htmldata}!urljoin@{urljoin}}
\index{urljoin@{urljoin}!htmldata@{htmldata}}
\subsubsection[{\texorpdfstring{urljoin(s, L)}{urljoin(s, L)}}]{\setlength{\rightskip}{0pt plus 5cm}def htmldata.\+urljoin (
\begin{DoxyParamCaption}
\item[{}]{s, }
\item[{}]{L}
\end{DoxyParamCaption}
)}\hypertarget{namespacehtmldata_a281fd13bfba54efd22dfe1e901181648}{}\label{namespacehtmldata_a281fd13bfba54efd22dfe1e901181648}
\begin{DoxyVerb}Write back document with modified URLs (reverses L{urlextract}).

Given a list C{L} of L{URLMatch} objects obtained from
L{urlextract}, substitutes changed URLs into the original
document C{s}, and returns the modified document.

One should only modify the C{.url} attribute of the L{URLMatch}
objects.  The ordering of the URLs in the list is not important.

 >>> doc = '<img src="a.png"><a href="b.png">'
 >>> L = urlextract(doc)
 >>> L[0].url = 'foo'
 >>> L[1].url = 'bar'
 >>> urljoin(doc, L)
 '<img src="foo"><a href="bar">'\end{DoxyVerb}
 

Definition at line 954 of file htmldata.\+py.

